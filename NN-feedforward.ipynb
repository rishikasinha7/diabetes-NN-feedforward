import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

import tensorflow as tf
df = pd.read_csv("diabetes.csv")
df.head(10)
df.columns
df['Outcome']==1
df[df['Outcome']==1]
len(df[df['Outcome']==1]),len(df[df['Outcome']==0])
## run through all columns except the last one for plotting histogram
for i in range(len(df.columns[:-1])): 
    label = df.columns[i]
    plt.hist(df[df['Outcome']==1][label], color = 'red', label = "Diabetic", alpha = 0.7, density = True, bins =15)
    plt.hist(df[df['Outcome']==0][label], color = 'green', label = "Not Diabetic", alpha = 0.7, density = True, bins =15)
    plt.title(label)
    plt.ylabel("Probability")
    plt.xlabel(label)
    plt.legend()
    plt.show()
    
X = df[df.columns[:-1]]
X
X = df[df.columns[:-1]].values
X
y = df[df.columns[-1]].values
y #1D ARRAY
X.shape, y.shape ## X is 2d and y is 1d
scaler = StandardScaler()
X = scaler.fit_transform(X)
data = np.hstack((X,np.reshape(y,(-1,1))))
transformed_df = pd.DataFrame(data, columns=df.columns)
X ## now everythig is a lot closer in range
## run through all columns except the last one for plotting histogram
for i in range(len(df.columns[:-1])): 
    label = df.columns[i]
    plt.hist(transformed_df[transformed_df['Outcome']==1][label], color = 'red', label = "Diabetic", alpha = 0.7, density = True, bins =15)
    plt.hist(transformed_df[transformed_df['Outcome']==0][label], color = 'green', label = "Not Diabetic", alpha = 0.7, density = True, bins =15)
    plt.title(label)
    plt.ylabel("Probability")
    plt.xlabel(label)
    plt.legend()
    plt.show()
### Training and Test datasets 

- scikits-learn's train_test_split //// sklearn.model_selection.train_test_split
- Split arrays and matrices into random train and test subsets


X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size = 0.4, random_state = 0)
#60% for training, 20-20% for validate and test 
#X_temp and y_temp are the X and y values but only 40% of them each
X_valid, X_test, y_valid, y_test = train_test_split(X_temp,y_temp, test_size = 0.5, random_state = 0)
### The Neural Net
# Class Sequential: Sequential groups a linear stack of layers into tf.keras.Model 
# (we need this as our NN is exacly like a stack of layers)
## dense nn - takes input from everything & outputs a value
## relu - if x <=0--> 0, if x>0-->x
## third layer has sigmoid function to map our input to a probability of whether or not something belongs to a single class
model = tf.keras.Sequential([
                                tf.keras.layers.Dense(16, activation = 'relu'),
                                tf.keras.layers.Dense(16, activation = 'relu'),
                                tf.keras.layers.Dense(1, activation = 'sigmoid')
])
# Compiling the Model
## choose from the tensorflow optimizers --> here Adam is used
## learning_rate = 0.001 is default
## Binary Classification is being done
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
             loss = tf.keras.losses.BinaryCrossentropy(),
             metrics =['accuracy']) 
Now we have a NN model that we can feed data to and train
-- before doing this we can see how our training and validation data perform
model.evaluate(X_train, y_train)
model.evaluate(X_valid, y_valid)
##fixing losses and accuracy
## batch_size : term that refers to the number of samples/training examples that is used in every single itiration
## validation_data : to checkwhat the validation loss and validation accuracy s=is after every epoch (epoch = the number of itirations)
model.fit(X_train, y_train, batch_size = 16, epochs = 20, validation_data = (X_valid, y_valid))
Our features are so different in range (BMI is 0-2.5; Skin thickness is 0 to 100, etc) that it could be messing up our results ----- so we will instead be scaling our results so that all of them are on a more standardized range --- import a package that lets us scale

--- see [99] 

